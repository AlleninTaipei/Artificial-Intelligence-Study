# Stanford Alpaca Study Note

## Introduction

[Alpaca 7B](https://crfm.stanford.edu/2023/03/13/alpaca.html) is an instruction-following model.

There are two important challenges to training a high-quality instruction-following model under an academic budget: **a strong pretrained language model** and **high-quality instruction-following data**. The first challenge is addressed with the recent release of Meta’s new [LLaMA](https://arxiv.org/abs/2212.10560) models. For the second challenge, the [self-instruct](https://arxiv.org/abs/2212.10560) paper suggests using an existing strong language model to automatically generate instruction data. In particular, Alpaca is a language model fine-tuned using supervised learning from a LLaMA 7B model on 52K instruction-following demonstrations generated from OpenAI’s text-davinci-003.

## Motivation and Challenges

Instruction-following models like GPT-3.5 (text-davinci-003), ChatGPT, Claude, and Bing Chat are powerful but still exhibit issues such as generating false information, social biases, and toxic language. Academic research on these models has been challenging due to the lack of accessible models comparable to proprietary systems like text-davinci-003. Alpaca aims to fill this gap by providing an accessible and research-oriented alternative.

## Technical Insights and Illustration

|Items|Description|
|-|-|
|Pretrained Model|Alpaca utilizes Meta’s LLaMA 7B model as its foundation.|
|Instruction-following data|Starting with the 175 human-written instruction-output pairs from the [self-instruct seed set](https://github.com/yizhongw/self-instruct), text-davinci-003 was prompted to generate more instructions using the seed set as in-context examples. The self-instruct method was improved by simplifying the [generation pipeline](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process) and significantly reducing the cost.|
|**Fine-Tuning**|Fine-tuning was performed using Hugging Face’s training framework with techniques like Fully Sharded Data Parallel and mixed precision training. **This process took 3 hours on 8 80GB A100 GPUs, costing under $100 on cloud compute platforms.**|

|Pipeline Illustration|Description|
|-|-|
|1. Seed Set Creation|Start with a small, manually created seed set of 175 instruction-output pairs.|
|2. Data Expansion|Use text-davinci-003 to expand the seed set, generating 52K unique instruction-output pairs.|
|3. Fine-Tuning|Fine-tune the LLaMA 7B model on the expanded dataset.|

|Training Efficiency|Details|
|-|-|
|Infrastructure|Utilized Hugging Face’s framework, Fully Sharded Data Parallel, and mixed precision training for efficient processing.|
|Cost|The fine-tuning process was cost-effective, with total costs kept below $600 by efficiently using cloud resources.|

## Dataset

alpaca_data.json contains 52K instruction-following data for fine-tuning the Alpaca model.

|a list of dictionaries|alpaca_data.json|
|-|-|
|`instruction`|`str`, describes the task the model should perform. Each of the 52K instructions is unique.|
|`input`|`str`, optional context or input for the task. For example, when the instruction is "Summarize the following article", the input is the article. Around 40% of the examples have an input.|
|`output`|`str`, the answer to the instruction as generated by text-davinci-003.|

        {
            "instruction": "Give three tips for staying healthy.",
            "input": "",
            "output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule."
        },

## Addressing OOM

**Naively, fine-tuning a 7B model requires about 7 x 4 x 4 = 112 GB of VRAM.**
Commands given above enable parameter sharding, so no redundant model copy is stored on any GPU. If you'd like to further reduce the memory footprint, here are some options:

**Turn on CPU offload for FSDP** with `--fsdp "full_shard auto_wrap offload"`.
This **saves VRAM at the cost of longer runtime**.

In our experience, DeepSpeed stage-3 (with offload) can at times be more memory efficient than FSDP with offload. Here's an example to use DeepSpeed stage-3 with 4 GPUs with both parameter and optimizer offload:
pip install deepspeed

        torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \
            --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \
            --data_path ./alpaca_data.json \
            --bf16 True \
            --output_dir <your_output_dir> \
            --num_train_epochs 3 \
            --per_device_train_batch_size 4 \
            --per_device_eval_batch_size 4 \
            --gradient_accumulation_steps 8 \
            --evaluation_strategy "no" \
            --save_strategy "steps" \
            --save_steps 2000 \
            --save_total_limit 1 \
            --learning_rate 2e-5 \
            --weight_decay 0. \
            --warmup_ratio 0.03 \
            --deepspeed "./configs/default_offload_opt_param.json" \
            --tf32 True

* The **DeepSpeed library** also provides some helpful functions to estimate memory usage.
**LoRA fine-tunes low-rank slices of the query, key, and value embedding heads. This can reduce the total memory footprint from 112GB to about 7x4=28GB.**

## Conclusion

Initially, Alpaca was made available through a public demo to disseminate research findings. However, due to hosting costs and content filter inadequacies, the demo was disabled after achieving its primary goal of enabling broad academic engagement.

Alpaca represents a significant step towards accessible and affordable instruction-following models in academia. By leveraging existing strong language models and innovative data generation methods, Alpaca demonstrates that high-quality instruction-following models can be developed with relatively low resources, promoting further research and development in this area.

---

## Code

### `generate_instruction.py`

This script is for generating instruction-following data using a language model like GPT-3 (specifically text-davinci-003). It uses human-written seed instructions and machine-generated instructions to create new ones. The process involves encoding prompts, generating responses, post-processing the responses, and filtering out irrelevant or inappropriate instructions.

|main components|Data Generation Process|
|-|-|
|`encode_prompt(prompt_instructions)`|Combines multiple prompt instructions into a single string, which is used as the input to the model.|
|`post_process_gpt3_response(num_prompt_instructions, response)`|Processes the raw output from the model, filtering out instructions based on various criteria (e.g., length, content).|
|`find_word_in_string(w, s)`|Checks if a word exists in a string.|
|`generate_instruction_following_data(...)`|The main function to generate the instruction-following data. It loads seed tasks, prepares the model input, generates responses, and filters the results. It also manages the progress and saves the generated data.|
|`main(task, **kwargs)`|Uses the fire library to run a specified task with provided arguments.|

* Below files cover a range of functionalities, including **machine learning model training**, **token management**, **dataset preparation**, **interaction with the OpenAI API**, and **JSON handling**.

### `train.py`

Sets up and trains a supervised fine-tuning model for a language model using the Hugging Face Transformers 

* Imports and Constants: Necessary libraries and modules are imported. Constants for token padding and prompt formats are defined.

* Dataclasses for Arguments: `ModelArguments`, `DataArguments`, and `TrainingArguments` dataclasses are defined to handle various configuration parameters.

|Functions|Description|
|-|-|
|`smart_tokenizer_and_embedding_resize`|Adjusts the tokenizer and the model's embedding layer to include any special tokens.|
|`_tokenize_fn`|Tokenizes a list of strings.|
|`preprocess`|Prepares the data by tokenizing sources and targets, ensuring proper formatting for training.|

* Dataset Class: `SupervisedDataset` is a custom dataset class that loads and preprocesses the training data from a given path, formats the inputs and outputs using predefined prompts, and tokenizes them.

* Data Collator: `DataCollatorForSupervisedDataset` is a custom data collator that pads sequences to the same length and creates the necessary input tensors for the model.

* Data Module Creation: `make_supervised_data_module` creates the dataset and data collator to be used by the trainer.

* Training Function: `train` function parses arguments, loads the model and tokenizer, adjusts the tokenizer and model for special tokens, prepares the data, and starts the training process using the Hugging Face `Trainer`.

* Main Execution:If the script is executed directly, the `train` function is called.

### `utils.py` 

Provides functions and classes for interacting with the OpenAI API, handling JSON files, and managing various OpenAI API decoding arguments.

* Import Statements: The script imports necessary modules for handling data classes, logging, mathematical operations, file I/O, and more.

* Constants and Initialization: It sets up an optional OpenAI organization from environment variables and logs this change if applicable.

* Data Classes: The OpenAIDecodingArguments data class defines arguments for decoding responses from the OpenAI API.

|Functions|Description|
|-|-|
|`openai_completion`|Generates completions from the OpenAI API based on provided prompts and decoding arguments.||
||Handles single or multiple prompts.|
||Batches prompts for efficient API requests.|
||Manages rate limits and retries on errors.|
||Optionally returns only the text of completions.|
|`_make_w_io_base` and `_make_r_io_base`|Helper functions to handle file I/O operations.|
|`jdump` , `jload`|Functions to dump and load JSON data.||
||`jdump` writes objects (dictionaries, lists, or strings) to a JSON file.|
||`jload` reads a JSON file into a dictionary.|
|To use the `openai_completion` function|`prompts = ["What is the capital of France?", "Who wrote 'To Kill a Mockingbird'?"]`<br>`decoding_args = OpenAIDecodingArguments(max_tokens=50, temperature=0.7)`<br><br>`completions = openai_completion(prompts, decoding_args, model_name="text-davinci-003", return_text=True)`<br>`print(completions)`|
|To save and load JSON data:|`data = {"key": "value"}`<br>`jdump(data, "data.json")`<br><br>`loaded_data = jload("data.json")`<br>`print(loaded_data)`|

### `weightdiff.py` 

Provides two main functionalities: creating a weight diff between a raw model and a tuned model, and recovering the original tuned weights from the diff and the raw model. The script uses the Hugging Face Transformers library, PyTorch, and the fire library for command-line interface generation.

Make Weight Diff: Computes the difference between the weights of a raw model and a tuned model, and saves this difference. This is useful for distributing model updates without sharing the entire model.

Recover Original Weights: Recovers the original tuned model's weights by adding the diff to the raw model's weights. This is useful for users who have the raw model and the weight diff and want to reconstruct the tuned model.

* Import Statements and Constants
`fire` for creating a command-line interface.
`torch` and `transformers` for model manipulation.
`tqdm` for progress bars.
`smart_tokenizer_and_embedding_resize` from the `train` module for handling tokenizers and embeddings.

* The `make_diff` function calculates the weight difference between a raw and a tuned model and saves the result.

* The `recover` function reconstructs the tuned model by adding the weight diff back to the raw model's weights:

* The `main` function uses the `fire` library to handle command-line arguments and execute the appropriate task (`make_diff` or `recover`):

### Summary

* The `train` function realizes the entire process from pre-trained model to fine-tuned model, ensuring optimized performance of the model on specific tasks.

|Tasks|Description|
|-|-|
|1.|Parsing command-line arguments.|
|2.|Loading pre-trained models and tokenizers.|
|3.|Handling special tokens of the tokenizer and adjusting the model's embedding matrix.|
|4.|Creating training datasets and data preprocessors.|
|5.|Initializing the trainer and conducting model fine-tuning.|
|6.|Saving the trained model and training status.|

* Through the `make_diff` function, we can calculate the weight difference between the fine-tuned model and the original model and save it as a differential file. This differential file can be distributed to others or used for version control.

|Key Point|Description|
|-|-|
|Reducing Storage and Transmission Costs|Fine-tuning a large model leads to a substantial increase in model size. Distributing the entire fine-tuned model consumes significant storage space and bandwidth. By distributing a weight differential file (diff), only the difference between the original and fine-tuned models needs to be transmitted and stored, significantly reducing file size and associated costs.|
|Intellectual Property Protection|Fine-tuning often involves proprietary data or techniques based on the original model. Distributing weight differential files protects the intellectual property of both the original model and the fine-tuning process. Recipients need the original model to reconstruct the complete fine-tuned model, preventing unauthorized use and distribution.
|Ensuring Model Consistency and Reproducibility|Weight differential files ensure reproducibility of the fine-tuning process. With the same original model and weight differential file, consistent fine-tuned models can be obtained through the recovery process. This is crucial for reproducibility of experimental results in research and development.|
|Version Control and Updates|Multiple versions of fine-tuned models may be produced during development and optimization. Storing and distributing weight differential files facilitates easier management and tracking of changes in different versions. By storing the original model and a series of differential files, all versions of fine-tuned models can be reconstructed without storing each version's complete model.|

* The `utils` function receives and parses input prompts and decoding parameters. It processes input prompts in batches and calls the OpenAI API to generate text batch by batch. It handles errors that may occur during API calls, such as rate limit errors, and performs appropriate retries. Depending on user requirements, it returns the generated text or the complete API response object. Through these steps, the`utils` function implements a flexible and reliable interface for invoking the OpenAI API for text generation.

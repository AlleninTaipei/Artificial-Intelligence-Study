# LLM Roadmap

|LLM Roadmap|Notes|
|-|-|
|LLM Engineering|Running LLMs<br>Building a Vector Storage<br>Retrieval Augmented Generation<br>Advanced RAG<br>Inference optimization<br>Deploying LLMs<br>Securing LLMs|
|LLM Scientist|The LLM architecture<br>Building an instruction dataset<br>Pre-training models<br>Supervised Fine-Tuning<br>Preference Alignment<br>Evaluation<br>Quantization<br>New Trends|
|LLM Fundamentals|Mathematics for Machine Learning<br>Python for Machine Learning<br>Neural Networks<br>Natural Language Processing (NLP)|

## LLM Engineering

|LLM Engineering|Notes|
|-|-|
|Running LLMs|LLM APIs<br>Open-source LLMs<br>Prompt engineering<br>Structuring outputs|
|Building a Vector Storage|Ingesting documents<br>Splitting documents<br>Embedding models<br>Vector databases|
|Retrieval Augmented Generation|Orchestrators<br>Retrievers<br>Memory<br>Evaluation|
|Advanced RAG|Query constructionAgents and tools<br>Post-processing<br>Program LLMs|
|Inference optimization|Flash AttentionKey-value cache<br>Speculative decoding|
|Deploying LLMs|Local deployment<br>Demo deployment<br>Server deployment<br>Edge deployment|
|Securing LLMs|Prompt hacking<br>Backdoors<br>Defensive measures|

## LLM Scientist

|LLM Scientist|Notes|
|-|-|
|The LLM architecture|High-level view<br>Tokenization<br>Attention mechanisms<br>Text generation|
|Building an instruction dataset|[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)<br>Advanced technique<br>Filtering data<br>Prompt templates|
|Pre-training models|Data pipeline<br>Causal language modeling<br>Scaling laws<br>High-Performance Computing|
|Supervised Fine-Tuning|Full fine-tuning<br>[LoRA](https://arxiv.org/abs/2106.09685)<br>[QLoRA](https://arxiv.org/abs/2305.14314)<br>[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)<br>[DeepSpeed](https://www.deepspeed.ai/)|
|Preference Alignment|Preference datasets<br>[Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)<br>[Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290)|
|Evaluation|Traditional metrics<br>General benchmarks<br>Task-specific benchmarks<br>Human evaluation|
|Quantization|Base techniques<br>GGUF and llama.cpp<br>AWQ|
|New Trends|Positional embeddings<br>Model merging<br>Mixture of Experts<br>Multimodal models|

## LLM Fundamentals

|LLM Fundamentals|Notes|
|-|-|
|Mathematics for Machine Learning|Linear Algebra<br>Calculus<br>Probability and Statistics|
|Python for Machine Learning|Python Basics<br>Data Science Libraries<br>Data Preprocessing<br>Machine Learning Libraries|
|Neural Networks|Fundamentals<br>Training and Optimization<br>Overfitting<br>Implement a Multilayer Perceptron (MLP)|
|Natural Language Processing (NLP)|Text Preprocessing<br>Feature Extraction Techniques<br>Word Embeddings<br>Recurrent Neural Networks (RNNs)|

## Sources

#### [Large Language Model Course - Maxime Labonne](https://github.com/mlabonne/llm-course)
#### [arXiv](https://arxiv.org/) is a community of volunteer authors, readers, moderators, advisory board members, supporting members, donors, and third-party collaborators that are supported by our staff at Cornell University.
#### The [Center for Research on Foundation Models](https://crfm.stanford.edu/) (CRFM) is an interdisciplinary initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that makes fundamental advances in the study, development, and deployment of foundation models.
